{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "565ffb05-8f43-4fb8-9baa-d23d9e6477a8",
    "_uuid": "0b8f73a9717768df79a909db36cc33054460a202"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8514a868-6816-494a-9e9c-dba14cb36f42",
    "_uuid": "bd2a68dcaace0c1371f2994f0204dce65800cc20"
   },
   "source": [
    "# A Data Science Framework\n",
    "\n",
    "1. **What is the problem?** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n",
    "2. **Where is the dataset?** John Naisbitt wrote in his 1984 book Megatrends, we are “drowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n",
    "3. **Prepare data for consumption.** This step is often referred to as data wrangling, a required process to turn “wild” data into “manageable” data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n",
    "4. **Perform exploratory analysis.** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n",
    "5. **Model Data** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that’s used as actionable intelligence) at worst.\n",
    "6. **Validate and Implement Data Model** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset.\n",
    "7. **Optimize, and Strategize** This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your “currency exchange\" rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22ba49b5-b3b6-4129-8c57-623e0d863ab3",
    "_uuid": "ff014109d795c5d1bfcc66c9761730406124ebe4"
   },
   "source": [
    "# Step 1: What is the problem?\n",
    "\n",
    "For this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n",
    "\n",
    "......\n",
    "\n",
    "**Project Summary:**\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    "Practice Skills\n",
    "* Binary classification\n",
    "* Python and R basics\n",
    "\n",
    "# Step 2: Where is the dataset?\n",
    "\n",
    "The dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\n",
    "\n",
    "# Step 3: Prepare data for consumption.\n",
    "\n",
    "Since step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef4059c6-9397-4b72-85da-570f51ba139c",
    "_uuid": "f1d8b59b37d2ce04a5396a0ab183dc3000817113"
   },
   "source": [
    "## 3.1 Import Libraries\n",
    "The following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "a33744a8-08c7-4158-a22f-e8f2008a25b0",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "412b006d19b0209bd10ee4c6d15fdf59f8d2af38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\n",
      "pandas version: 0.20.3\n",
      "matplotlib version: 2.1.0\n",
      "NumPy version: 1.13.3\n",
      "SciPy version: 0.19.1\n",
      "IPython version: 6.1.0\n",
      "scikit-learn version: 0.19.1\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format( sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format( pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format( matplotlib.__version__)) \n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format( np.__version__)) \n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format( sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format( IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format( sklearn.__version__))\n",
    "\n",
    "#Misc Libraries\n",
    "import timeit\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../data/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../data\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-911f4fad6996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_ouput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "help(subprocess.check_ouput())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a514e15b-8caa-4b2e-bb46-13521761ef27",
    "_uuid": "2e4810f3c85ffdee7d7382167e455baf1c320bc8"
   },
   "source": [
    "## 3.11 Load Data Modelling Libraries\n",
    "\n",
    "We will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b230c790-a3f5-46ed-b351-57e96cc8fa61",
    "_uuid": "4a16d09256317518769f21bf9cc38726df8b0078",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "# Configure visualizations\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 12 , 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c6289058-157a-46ba-b5c3-f134b268f5cc",
    "_uuid": "a0fe0965de515e8c82cabd1fbd82cce993279f16"
   },
   "source": [
    "## 3.2 Meet and Great Data\n",
    "\n",
    "This is the meet and great step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n",
    "\n",
    "To begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https://www.kaggle.com/c/titanic/data).\n",
    "\n",
    "1. The *Survived* variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are potential predictor or independent variables. **It's important to note, more predictor variables do not make a better model, but the right variables.**\n",
    "2. The *PassengerID* and *Ticket* variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.\n",
    "3. The *Pclass* variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper class, 2 = middle class, and 3 = lower class.\n",
    "4. The *Name* variable is a nominal datatype. It could be used for feature engineering to derive the gender from title, family size from surname, and SES from titles like doctor or master. Since these variables already exist, we'll make use of it to see if title, like master, makes a difference.\n",
    "5. The *Sex* and *Embarked* variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.\n",
    "6. The *Age* and *Fare* variable are continuous quantitative datatypes.\n",
    "7. The *SibSp* represents number of related siblings/spouse aboard and *Parch* represents number of related parents/children aboard. Both are discrete quantitative datatypes. This can be used for feature engineering to create a family size and is alone variable.\n",
    "8. The *Cabin* variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels. However, since there are many null values, it does not add value and thus is excluded from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "c0928d7d-043b-4aa5-8113-e96fb42dbdc7",
    "_uuid": "d0afb79a714ea826208235d05afac4e3f60554db"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../data/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a8489bfb7e6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#a dataset should be broken into 3 splits: train, test, and (final) validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'../data/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "data_raw = pd.read_csv('../data/train.csv')\n",
    "\n",
    "\n",
    "#a dataset should be broken into 3 splits: train, test, and (final) validation\n",
    "#the test file provided is the validation file for competition submission\n",
    "#we will split the train set into train and test data in future sections\n",
    "data_val  = pd.read_csv('../data/test.csv')\n",
    "\n",
    "\n",
    "#to play with our data we'll create a copy\n",
    "#remember python assignment or equal passes by reference vs values, so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\n",
    "data1 = data_raw.copy(deep = True)\n",
    "\n",
    "#however passing by reference is convenient, because we can clean both datasets at once\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "\n",
    "#preview data\n",
    "print (data_raw.info()) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\n",
    "#data_raw.head() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n",
    "#data_raw.tail() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n",
    "data_raw.sample(10) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b575d2d8-b77d-4810-9232-3b2726145ca8",
    "_uuid": "281ad467b485855c1a39116c5ed899ac3018689f"
   },
   "source": [
    "## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\n",
    "\n",
    "In this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n",
    "\n",
    "1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n",
    "2. **Completing:** There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because it may skew some algorithms. Thus, it's important to fix before we start modeling. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the model’s accuracy.\n",
    "3. **Creating:**  Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n",
    "4. **Converting:** Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e2181a68-3c33-4040-83a9-99adee8363a0",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "9a36681f0eda8ccc6396c64aa6bd6e8288461bcb",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c6fc9f66-8779-454c-b6bd-47676c0dc81c",
    "_uuid": "5002c97888cfdcb2b49ef76fd1505eed6a1721e4"
   },
   "source": [
    "## 3.22 Clean Data\n",
    "\n",
    "Now that we know what to clean, let's execute our code.\n",
    "\n",
    "** Source Documentation: **\n",
    "* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n",
    "* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n",
    "* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n",
    "* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "* [pandas.isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n",
    "* [pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html)\n",
    "* [pandas.DataFrame.mode](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html)\n",
    "* [pandas.DataFrame.copy](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)\n",
    "* [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)\n",
    "* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n",
    "* [pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56e3e982-a23a-4c84-b8a1-976adc94aa0b",
    "_uuid": "160dd59e4ebe7a94521c65da2746ca370b4bd694",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###COMPLETING: complete or delete missing values in train and test/validation dataset\n",
    "for dataset in data_cleaner:    \n",
    "    #complete missing age with median\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "\n",
    "    #complete embarked with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "    #complete missing age with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "#delete the cabin feature/column and others previously stated to exclude in train dataset\n",
    "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
    "data1.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "print(data1.isnull().sum())\n",
    "print(data_val.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c66b14b7-e1e5-43d7-9260-840fe0ef46c7",
    "_uuid": "e62f1b14f6f9758b66182e2476f3c57bb4a0734c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###CREATE: Feature Engineering for train and test/validation dataset\n",
    "for dataset in data_cleaner:    \n",
    "    #Discrete variables\n",
    "    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "\n",
    "\n",
    "    #Continuous variable bins; qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n",
    "    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "\n",
    "    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "\n",
    "\n",
    "    \n",
    "#cleanup rare title names\n",
    "#print(data1['Title'].value_counts())\n",
    "stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "title_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n",
    "\n",
    "#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "print(data1['Title'].value_counts())\n",
    "\n",
    "\n",
    "#preview data again\n",
    "data1.info()\n",
    "data_val.info()\n",
    "data1.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f7972928-84b5-47b1-98b6-3e47bc7ddb17",
    "_uuid": "eb0a15c2065a827c3c5431622c04707f3e9d2e52"
   },
   "source": [
    "## 3.23 Convert Formats\n",
    "\n",
    "We will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n",
    "\n",
    "In this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n",
    "\n",
    "** Source Documentation: **\n",
    "* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n",
    "* [Sklearn LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "* [Sklearn OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "* [Pandas Categorical dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html)\n",
    "* [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "95f9a05d-9f6e-46b3-8a3c-95a78f0c0834",
    "_uuid": "7f746d8c6f7d0214253919185fa16f2f6a447a15",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n",
    "\n",
    "#code categorical data\n",
    "label = LabelEncoder()\n",
    "for dataset in data_cleaner:    \n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "\n",
    "\n",
    "#define y variable aka target/outcome\n",
    "Target = ['Survived']\n",
    "\n",
    "#define x variables for original features aka feature selection\n",
    "data1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\n",
    "data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "\n",
    "#define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "data1_dummy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "43ae9ddf-85b6-4abd-89a7-cfb0ff70ebad",
    "_uuid": "9f4b880b2e485e6bc4b6a64113b8f816e2319e85"
   },
   "source": [
    "## 3.24 Da-Double Check Cleaned Data\n",
    "\n",
    "Now that we've cleaned our data, let's do a discount da-double check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00d10c37-5d8a-4eaf-8fe8-25a0b58e0bfd",
    "_uuid": "e2ef29db7e31dc83c10238ee33ad4ad0ad426183",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e1ffcb68-17a5-43e0-b7e1-4a0b9d33dbdd",
    "_uuid": "9b4b6e1f40e310274447c880760b87ec6a0c7c77"
   },
   "source": [
    "## 3.25 Split Training and Testing Data\n",
    "\n",
    "As mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't overfit our model. Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the same dataset. It's important our algorithm has not seen the subset we will use to test, so it doesn't \"cheat\" by memorizing the answers. We will use [*sklearn* train_test_split function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Or you can use [*sklearn* cross validation functions](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15e84f99-e015-4e0a-bd0a-2f856ffad1f6",
    "_uuid": "f9eb9f6e78235a38580ee3125ede17b836edabb3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split train and test data with function defaults\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target])\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target])\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target])\n",
    "\n",
    "\n",
    "print(\"Data Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a13c80c-9f6b-405a-bba2-d3fb9b1cf19d",
    "_uuid": "d3d49dc8106082d149dc4058b002355fc809726b"
   },
   "source": [
    "# Step 4: Perform Exploratory Analysis with Descriptive Statistics\n",
    "\n",
    "Now that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f14819d3-e23c-40fa-a281-375e8777a47b",
    "_uuid": "5355b0e45b130a1fd574510141f65e62ebfe2148",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Discrete Variable Correlation by Survival using\n",
    "#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64' :\n",
    "        print('Survival Correlation by:', x)\n",
    "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "        print('-'*10, '\\n')\n",
    "        \n",
    "\n",
    "#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\n",
    "print(pd.crosstab(data1['Title'],data1[Target[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ada8f356-f5bf-4629-81c9-b5a95e16a09d",
    "_uuid": "a8485baedcf8d61ab13c1cb19d3654a835ce1765",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n",
    "\n",
    "#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "\n",
    "#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n",
    "#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n",
    "#and subplotS: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html?highlight=matplotlib%20pyplot%20subplots#matplotlib.pyplot.subplots\n",
    "\n",
    "#graph distribution of quantitative data\n",
    "plt.figure(figsize=[16,12])\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\n",
    "plt.title('Fare Boxplot')\n",
    "plt.ylabel('Fare ($)')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.boxplot(data1['Age'], showmeans = True, meanline = True)\n",
    "plt.title('Age Boxplot')\n",
    "plt.ylabel('Age (Years)')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\n",
    "plt.title('Family Size Boxplot')\n",
    "plt.ylabel('Family Size (#)')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Fare Histogram by Survival')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Age Histogram by Survival')\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Family Size Histogram by Survival')\n",
    "plt.xlabel('Family Size (#)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n",
    "\n",
    "#graph individual features by survival\n",
    "fig, saxis = plt.subplots(2, 3,figsize=(16,12))\n",
    "\n",
    "sns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\n",
    "sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\n",
    "sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n",
    "\n",
    "sns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0])\n",
    "sns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1])\n",
    "sns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2])\n",
    "\n",
    "\n",
    "#graph distribution of qualitative data: Pclass\n",
    "#we know class mattered in survival, now let's compare class and a 2nd feature\n",
    "fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(16,12))\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\n",
    "axis1.set_title('Pclass vs Fare Survival Comparison')\n",
    "\n",
    "sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\n",
    "axis2.set_title('Pclass vs Age Survival Comparison')\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)\n",
    "axis3.set_title('Pclass vs Family Size Survival Comparison')\n",
    "\n",
    "\n",
    "#graph distribution of qualitative data: Sex\n",
    "#we know sex mattered in survival, now let's compare sex and a 2nd feature\n",
    "fig, qaxis = plt.subplots(1,3,figsize=(16,12))\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\n",
    "axis1.set_title('Sex vs Embarked Survival Comparison')\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\n",
    "axis1.set_title('Sex vs Pclass Survival Comparison')\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])\n",
    "axis1.set_title('Sex vs IsAlone Survival Comparison')\n",
    "\n",
    "\n",
    "#more side-by-side comparisons\n",
    "fig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(16,12))\n",
    "\n",
    "#how does family size factor with sex & survival\n",
    "sns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n",
    "              palette={\"male\": \"blue\", \"female\": \"pink\"},\n",
    "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n",
    "\n",
    "#how does class factor with sex & survival\n",
    "sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n",
    "              palette={\"male\": \"blue\", \"female\": \"pink\"},\n",
    "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)\n",
    "\n",
    "\n",
    "#how does embark port factor with class, sex, and survival\n",
    "#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n",
    "e = sns.FacetGrid(data1, col = 'Embarked')\n",
    "e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\n",
    "e.add_legend()\n",
    "\n",
    "\n",
    "#plot distributions of Age of passengers who survived or did not survive\n",
    "a = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\n",
    "a.map(sns.kdeplot, 'Age', shade= True )\n",
    "a.set(xlim=(0 , data1['Age'].max()))\n",
    "a.add_legend()\n",
    "\n",
    "\n",
    "#histogram\n",
    "h = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\n",
    "h.map(plt.hist, 'Age', alpha = .75)\n",
    "h.add_legend()\n",
    "\n",
    "#pair plots\n",
    "pp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])\n",
    "\n",
    "\n",
    "#correlation heatmap\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "079d255c-e55e-482b-be9f-ae6b23015790",
    "_uuid": "1f4f5f75093fdcec864a835d94cd5d9a05b591c5"
   },
   "source": [
    "# Step 5: Model Data\n",
    "\n",
    "Data Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that disciple. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don’t worry, we only need a high-level overview, which we’ll cover in this Kernel. Also, thanks to computer science, a lot of the heavy lifting is done for you. So, problems that once required graduate degrees in mathematics or statistics, now only take a few lines of code. Last, we’ll need some business acumen to think through the problem. After all, like training a sight-seeing dog, it’s learning from us and not the other way around.\n",
    "\n",
    "Machine Learning (ML), as the name suggest, is teaching the machine how-to do something. While this topic and big data has been around for decades, it is becoming more popular than ever because the barrier to entry is lower. This is both good and bad. It’s good because these algorithms are now accessible to more people that can solve more problems in the real world. It’s bad because a lower barrier to entry means more people will not know the tools they are using and can come to incorrect conclusions. That’s why I focus on teaching you, not just what to do, but why you’re doing it. Previously, I used the analogy of asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible; or even worst, implements incorrect actionable intelligence. So now that I’ve hammered (no pun intended) my point, I’ll show you what to do and most importantly, WHY you do it.\n",
    "\n",
    "First, you must understand, that the purpose of machine learning is to solve human problems. Machine learning can be categorized as: supervised learning, unsupervised learning, and reinforced learning. Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer Unsupervised learning is where you train the model using a training dataset that does not include the correct answer. And reinforced learning is a hybrid of the previous two, where the model is not given the correct answer immediately, but later after a sequence of events to reinforce learning. We are doing supervised machine learning, because we are training our algorithm by presenting it with a set of features and their corresponding target. We then hope to present it a new subset from the same dataset and have similar results in prediction accuracy.\n",
    "\n",
    "There are many machine learning algorithms, however they can be reduced to four categories: classification, regression, clustering, or dimensionality reduction, depending on your target variable and data modeling goals. We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. One side note, logistic regression, while it has regression in the name, is really a classification algorithm. Since our problem is predicting if a passenger survived or did not survive, this is a discrete target variable. We will use a classification algorithm from the *sklearn* library to begin our analysis.\n",
    "\n",
    "**Machine Learning Selection**\n",
    "* [Sklearn Estimator Overview](http://scikit-learn.org/stable/user_guide.html)\n",
    "* [Sklearn Estimator Detail](http://scikit-learn.org/stable/modules/classes.html)\n",
    "* [Choosing Estimator Mind Map](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "* [Choosing Estimator Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "\n",
    "\n",
    "Now that we identified our solution as a supervised learning classification algorithm. We can narrow our list of choices.\n",
    "\n",
    "**Machine Learning Classification Algorithms:**\n",
    "* [Ensemble Methods](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Generalized Linear Models (GLM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Naive Bayes](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)\n",
    "* [Nearest Neighbors](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)\n",
    "* [Support Vector Machines (SVM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
    "* [Decision Trees](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68816e33-19bc-482e-b4e8-062098b18798",
    "_uuid": "979ae81e95cf760f05e94853b932b5101198b4e9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(n_estimators = 100),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model. RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(n_neighbors = 3),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    ]\n",
    "\n",
    "#create table to compare MLA\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy', 'MLA Test Accuracy', 'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = 10)\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = cv_results['test_score'].mean()   \n",
    "\n",
    "    MLA[row_index] = alg.fit (data1[data1_x_bin], data1[Target]) #fit model for submission\n",
    "\n",
    "    row_index+=1\n",
    "\n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)\n",
    "print(MLA_compare)\n",
    "MLA_compare\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b9653051-8e79-4ae3-ac31-497bf0a5caed",
    "_uuid": "b7acbf3f7ad6e812a2b5d60794bc3c5ad7d3fd0c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c90ba48e-3c44-47c8-9bd2-d46187efb9d8",
    "_uuid": "9e3ac7492a21e7f60b0656008f7873cab882f196"
   },
   "source": [
    "## 5.1 Evaluate Model Performance\n",
    "\n",
    "Let's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~85% accuracy. If this were a college course, that would be a B-grade. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of model optimization. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So, keep that in mind.\n",
    "\n",
    "For model optimization, we have a couple options: 1) find a \"better\" algorithm, 2) tune our current algorithm parameters, 3) feature engineer new variables to find new signals in the data, or 4) we can go back to the beginning and determine if we asked the right questions, got the right data, and made the right decisions along the process.\n",
    "\n",
    "### Data Science 101: Determine a Baseline Accuracy ###\n",
    "Before we decide how-to make our model better, let's determine if ~85% is good enough. To do that, we have to go back to the basics of data science 101. We know this is a binary problem, because there are only two possible outcomes; passengers survived or died. So, think of it like a coin flip. If you have a fair coin and you guessed heads or tail, then you have a 50-50 chance of guessing right. So, let's set 50% as an F grade, because if your model accuracy is any worse than that, then why do I need you when I can just flip a coin?\n",
    "\n",
    "Okay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 1,502/2,224 or 67.5% of people died. Therefore, If I just guessed that 100% of people died, then I would be right 67.5% of the time. So, let's set 68% as a D grade, because again, if your model accuracy is any worst that that, then why do I need you, when I can just assume if you were on the Titanic that day you died and have a 68% accuracy.\n",
    "\n",
    "### Data Science 101: How-to Create Your Own Model ###\n",
    "Our accuracy is increasing, but can we do better? Are there any signals in our data? To illustrate this, we're going to build our own decision tree model, because it is the easiest to conceptualize and requires simple addition and multiplication calculations. When creating a decision tree, you want to ask questions that gives you better information about your outcome by segregated the survived/1 from the dead/0. This is part science and part art, so let's just play the 21-question game to show you how it works. If you want to follow along on your own, download the train dataset and import into Excel. Create a pivot table with survival in the columns, count and % of row count in the values, and the features described below in the rows.\n",
    "\n",
    "Remember, the name of the game is to create subsets using a decision tree model to get survived/1 in one bucket and dead/0 in another bucket. Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived/1, but if 50% or less survived then if everybody in our subgroup died/0. Also, we will stop if the subgroup is 10% of our total dataset or 9 cases and/or our model accuracy plateaus or decreases. Got it? Let's go!\n",
    "\n",
    "***Question 1: Were you on the Titanic?*** If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.\n",
    "\n",
    "***Question 2: Are you male or female?*** Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.\n",
    "\n",
    "***Question 3A (going down the female branch with count = 344): Are you in class 1, 2, or 3?*** Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since are dead group is less than 9, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.\n",
    "\n",
    "***Question 4A (going down the female class 3 branch with count = 144): Did you embark from port C, Q, or S?*** We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is 9 or less, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%. \n",
    "\n",
    "***Question 5A (going down the female class 3 embarked S branch with count = 88):*** So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.\n",
    "\n",
    "***Question 3B (going down the male branch with count = 577):*** Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter, title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.\n",
    "\n",
    "So, with very little information, we get to 82% accuracy. We'll give that a grade of a C for average or our baseline. But can we do better than our handmade model? By making a better tree, new features, etc.  \n",
    "\n",
    "Before we do, let's code what we just wrote above. Please note, this is the manual process by \"hand.\" You won't have to do this, but it's important to understand it before you start working with MLA. Think of MLA like a TI-89 calculator on a Calculus Exam. It's very powerful and helps you with a lot of the grunt work. But if you don't know what you're doing on the exam, a calculator, even a TI-89, is not going to help you pass. So study the next section wisely.\n",
    "\n",
    "Reference: [Cross-Validation and Decision Tree Tutorial](http://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC411/tutorial3_CrossVal-DTs.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "330582d5-8034-4c4a-93f0-60067939c5c0",
    "_uuid": "82a3b4a54c2cb510f790ab2f7e3c52c46b63206b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORTANT: This is a handmade model for learning purposes only.\n",
    "#However, it is possible to create your own predictive model without a fancy algorithm :)\n",
    "\n",
    "#coin flip model with random 1/survived 0/died\n",
    "\n",
    "#Iterate over DataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\n",
    "for index, row in data1.iterrows(): \n",
    "    #random number generator: https://docs.python.org/2/library/random.html\n",
    "    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n",
    "        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n",
    "    else: \n",
    "        data1.set_value(index, 'Random_Predict', 0) #predict died/0\n",
    "    \n",
    "\n",
    "#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n",
    "#the mean of the column will then equal the accuracy\n",
    "data1['Random_Score'] = 0 #assume prediction wrong\n",
    "data1.loc[(data1['Survived'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\n",
    "print('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n",
    "\n",
    "#we can also use scikit's accuracy_score function to save us a few lines of code\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "print('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1['Survived'], data1['Random_Predict'])*100))\n",
    "\n",
    "\n",
    "\n",
    "#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "pivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\n",
    "print('\\n\\nSurvival Decision Tree w/Female Node: \\n',pivot_female)\n",
    "\n",
    "pivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\n",
    "print('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n",
    "\n",
    "\n",
    "#handmand data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\n",
    "def mytree(df):\n",
    "    \n",
    "    #initialize table to store predictions\n",
    "    Model = pd.DataFrame(data = {'Predict':[]})\n",
    "    male_title = ['Master', 'Sir'] #survived titles\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        #Question 1: Were you on the Titanic; majority died\n",
    "        Model.loc[index, 'Predict'] = 0\n",
    "\n",
    "        #Question 2: Are you female; majority survived\n",
    "        if (df.loc[index, 'Sex'] == 'female'):\n",
    "                  Model.loc[index, 'Predict'] = 1\n",
    "\n",
    "        #Question 3A Female - Class and Question 4 Embarked gain minimum information\n",
    "\n",
    "        #Question 5B Female - FareBin; set anything less than .5 in female node decision tree back to 0\n",
    "        if ((df.loc[index, 'Sex'] == 'female') & \n",
    "            (df.loc[index, 'Pclass'] == 3) & \n",
    "            (df.loc[index, 'Embarked'] == 'C') & \n",
    "            (df.loc[index, 'Fare'] in range (8,15))\n",
    "           ):\n",
    "                  Model.loc[index, 'Predict'] = 0              \n",
    "        \n",
    "        if ((df.loc[index, 'Sex'] == 'female') & \n",
    "            (df.loc[index, 'Pclass'] == 3) & \n",
    "            (df.loc[index, 'Embarked'] == 'S')  &\n",
    "            (df.loc[index, 'Fare'] > 8)\n",
    "\n",
    "           ):\n",
    "                  Model.loc[index, 'Predict'] = 0\n",
    "\n",
    "        #Question 3B Male: Title; set anything greater than .5 to 1 for majority survived\n",
    "        if ((df.loc[index, 'Sex'] == 'male') &\n",
    "            (df.loc[index, 'Title'] in male_title)\n",
    "            ):\n",
    "            Model.loc[index, 'Predict'] = 1\n",
    "        \n",
    "        \n",
    "    return Model\n",
    "\n",
    "\n",
    "#model data\n",
    "Tree_Predict = mytree(data1)\n",
    "print('\\n\\nDecision Tree Model Accuracy: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))\n",
    "\n",
    "\n",
    "#Accuracy Summary Report with http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "#Where recall score = (true positives)/(true positive + false negative) w/1 being best:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "#And F1 score = weighted average of precision and recall w/1 being best: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "print(metrics.classification_report(data1['Survived'], Tree_Predict))\n",
    "\n",
    "#Plot Accuracy Summary\n",
    "#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(data1['Survived'], Tree_Predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = ['Dead', 'Survived']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n",
    "                      title='Normalized confusion matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec85d77e-0be8-457e-b876-ccf3545d35e5",
    "_uuid": "9e6bf62b0e2a7f8ce2cccd68310338dfc5d2c156"
   },
   "source": [
    "## 5.11 Model Performance with Cross-Validation (CV)\n",
    "In step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n",
    "\n",
    "Remember, it's important we have a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%, but then when you go to take the exam, you fail because you never truly learned anything. The same is true with machine learning.\n",
    "\n",
    "However, when we have a relatively small dataset, having to split our data into two or more subsets, can hurt our model because you start to lose significant information. CV is basically a shortcut to split our data multiple times and take the average of the iterations. It’s a little more expensive in computer processing, but it is often used to derive a more accurate estimate of model performance.\n",
    "\n",
    "![CV](http://blog-test.goldenhelix.com/wp-content/uploads/2015/04/B-fig-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ae78385-67ec-431e-98ad-1c34ab10499b",
    "_uuid": "68978ae17037762ad30405bb51ec67326fecfdec"
   },
   "source": [
    "# 5.12 Tune Model with Hyper-Parameter Optimizers\n",
    "\n",
    "When we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves the opportunity to see how various parameter settings will change the model accuracy.\n",
    "\n",
    "However, in order to tune a model, we need to actually understand it. That's why I took the time in the previous section to show you  how predictions work. Now let's learn a little bit more about our DT algorithm.\n",
    "\n",
    "Credit: [sklearn](http://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "\n",
    ">**Some advantages of decision trees are:**\n",
    "* Simple to understand and to interpret. Trees can be visualised.\n",
    "* Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "* Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n",
    "* Able to handle multi-output problems.\n",
    "* Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "* Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "> **The disadvantages of decision trees include:**\n",
    "* Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "* The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "\n",
    "\n",
    "Below are available parameters and [defintions](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier):\n",
    "> class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
    "\n",
    "\n",
    "We will tune our model using [ParameterGrid](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html#sklearn.model_selection.ParameterGrid) and [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). We will then visualize our tree with [graphviz](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a614a6a0-1861-4d87-aaab-cc8f9b54bd82",
    "_uuid": "4cfe6cb996dd39e1145effffa51a6953c396278d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#base model\n",
    "dtree = MLA[15]\n",
    "print('BEFORE DT Parameters: ', dtree.get_params())\n",
    "print(\"BEFORE DT Training w/bin set score: {:.2f}\". format(dtree.score(train1_x_bin, train1_y_bin)*100)) \n",
    "print(\"BEFORE DT Test w/bin set score: {:.2f}\". format(dtree.score(test1_x_bin, test1_y_bin)*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "#tune parameters\n",
    "param_grid = {'criterion': ['gini', 'entropy'], \n",
    "              'splitter': ['best', 'random'],\n",
    "              'max_depth': [None, 2,4,6,8,10],\n",
    "              'min_samples_split': [5,10,15,20,25],\n",
    "              'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "             }\n",
    "\n",
    "#print(list(model_selection.ParameterGrid(param_grid)))\n",
    "\n",
    "#best model; can use full training set because includes CV\n",
    "tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'accuracy', cv = 5)\n",
    "tune_model = tune_model.fit (data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print('AFTER DT Parameters: ', tune_model.get_params())\n",
    "print(\"AFTER DT Training w/bin set score: {:.2f}\". format(tune_model.score(train1_x_bin, train1_y_bin)*100)) \n",
    "print(\"AFTER DT Test w/bin set score: {:.2f}\". format(tune_model.score(test1_x_bin, test1_y_bin)*100))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e84f865-d316-4268-a422-c0062e4069c2",
    "_uuid": "cf8c9846dfcbd7277756f1a573d55642d3f2f2be"
   },
   "source": [
    "## 5.13 Feature Selection with RFECV\n",
    "As stated in the beginning, more predictor variables do not make for a better model, but the right predictors do. So, another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV), so we can model on our full train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df54f00c-e5a2-4994-aab8-f5b19961eafb",
    "_uuid": "d35bf5c10d2dc9c582e5fea4956942182c6fd206",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tuned model\n",
    "print('BEFORE DT Parameters: ', tune_model.get_params())\n",
    "print(\"BEFORE DT Training w/bin set score: {:.2f}\". format(tune_model.score(train1_x_bin, train1_y_bin)*100)) \n",
    "print(\"BEFORE DT Test w/bin set score: {:.2f}\". format(tune_model.score(test1_x_bin, test1_y_bin)*100))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "dtree_rfe = feature_selection.RFECV(tree.DecisionTreeClassifier(), step = 1, cv = 10)\n",
    "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
    "X_rfe = dtree_rfe.transform(data1[data1_x_bin])\n",
    "\n",
    "print('Train Shape New: ', X_rfe.shape) \n",
    "print('Train Columns New: ', train1_x_bin.columns.values[dtree_rfe.get_support()])\n",
    "print('-'*10)\n",
    "\n",
    "print('AFTER DT Parameters: ', dtree_rfe.get_params())\n",
    "print(\"AFTER DT Training w/bin set score: {:.2f}\". format(dtree_rfe.score(train1_x_bin, train1_y_bin)*100)) \n",
    "print(\"AFTER DT Test w/bin set score: {:.2f}\". format(dtree_rfe.score(test1_x_bin, test1_y_bin)*100))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5737627-e5ea-480f-883b-eea1caaa0e46",
    "_uuid": "7337e6c91797354d8fd887a538bf916a80619069",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                                feature_names = data1_x_bin, class_names = True,\n",
    "                                filled = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f9bd02f-93ae-4cfa-992c-38b24c322011",
    "_uuid": "6e2d288d4e477837f513e37461754256e1eef44a"
   },
   "source": [
    "# Step 6: Validate and Implement\n",
    "The next step is to prepare for submission using the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "057f369f-a3c6-4718-94f4-db8cb6d59777",
    "_uuid": "29ad86c5413910763316df7fda7753c4b0d0a97d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for modeling\n",
    "print(data_val.info())\n",
    "print(\"-\"*10)\n",
    "#data_val.sample(10)\n",
    "\n",
    "\n",
    "#model and predict\n",
    "#submision 1 decision tree base\n",
    "#print(dtree)\n",
    "\n",
    "#submission 2 decision tree tuned\n",
    "#print(tune_model)\n",
    "\n",
    "#submission 3 svc default\n",
    "#print(MLA[13])\n",
    "\n",
    "#submission 4 handmade decision tree\n",
    "\n",
    "data_val['Survived'] = mytree(data_val).astype(int)\n",
    "submit = data_val[['PassengerId','Survived']]\n",
    "submit.to_csv(\"../working/submit.csv\", index=False)\n",
    "\n",
    "print('Validation Data Distribution: \\n', data_val['Survived'].value_counts(normalize = True))\n",
    "submit.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5678e859-6610-4895-8fe8-0d28f0767b25",
    "_uuid": "b04d314d7ffa7e1d48b3ab6a90004fcee56b0ea9"
   },
   "source": [
    "# Credits\n",
    "Programming is all about \"borrowing\" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. \n",
    "\n",
    "* Müller, Andreas C.; Guido, Sarah. Introduction to Machine Learning with Python: A Guide for Data Scientists. O'Reilly Media.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51f197c2-76d7-4503-ade5-f9f35323c4c5",
    "_uuid": "2e95a403934cf5c0ab1ff9148b62ef14590e84c9"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
